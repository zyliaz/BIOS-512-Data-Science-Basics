{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d684172-4b94-42cf-bda2-e11952420d86",
   "metadata": {},
   "source": [
    "# Homework 10\n",
    "#### Course Notes\n",
    "**Language Models:** https://github.com/rjenki/BIOS512/tree/main/lecture17  \n",
    "**Unix:** https://github.com/rjenki/BIOS512/tree/main/lecture18  \n",
    "**Docker:** https://github.com/rjenki/BIOS512/tree/main/lecture19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839a5ba-62f4-4699-baea-018afda70786",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "#### Make a language model that uses ngrams and allows the user to specify start words, but uses a random start if one is not specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef37d3a-a6ad-42ae-9e16-7d7338c9ce49",
   "metadata": {},
   "source": [
    "#### a) Make a function to tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965cad0c-e897-422a-b147-84f3c02c3abf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "also installing the dependency ‘SnowballC’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/0q/ndh_pk0s37n95y6svtjfy7hw0000gp/T//RtmpekCzRA/downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"tokenizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81d45571-6836-4d6d-8a96-951e5e73889f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "library(httr)\n",
    "library(tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb52da59-ed74-4460-943e-649c27e339c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenize_text <- function(text) {\n",
    "    tokenizers::tokenize_words(text, lowercase=TRUE, strip_punct=TRUE)[[1]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86145513-294b-4894-a02c-8ae60e2c616e",
   "metadata": {},
   "source": [
    "#### b) Make a function generate keys for ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34243105-9ea0-4f96-86ef-2ee837b865e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_from <- function(ngram, sep = \"\\x1f\") {\n",
    "    paste(ngram, collapse=sep)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52988c2c-b230-467f-b519-72bc85b93b43",
   "metadata": {},
   "source": [
    "#### c) Make a function to build an ngram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "643022c9-6368-457e-a6a9-9a49421c513d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_ngram_table <- function(tokens, n, sep = \"\\x1f\") {\n",
    "    if (length(tokens) < n) return(new.env(parent = emptyenv()))\n",
    "    tbl <- new.env(parent = emptyenv())\n",
    "    for (i in seq_len(length(tokens) - n + 1L)) {\n",
    "        ngram <- tokens[i:(i + n - 2L)]\n",
    "        next_word <- tokens[i + n - 1L]\n",
    "        key <- paste(ngram, collapse = sep)\n",
    "        counts <- if (!is.null(tbl[[key]])) tbl[[key]] else integer(0)\n",
    "        if (next_word %in% names(counts)) {\n",
    "            counts[[next_word]] <- counts[[next_word]] + 1L\n",
    "        } else {\n",
    "            counts[[next_word]] <- 1L\n",
    "        }\n",
    "        tbl[[key]] <- counts\n",
    "    }\n",
    "    tbl\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6db37-abce-4705-9784-e1b898174f00",
   "metadata": {},
   "source": [
    "#### d) Function to digest the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f5f7fee-d455-4309-9c36-e65d172505c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "digest_text <- function(text, n) {\n",
    "    tokens <- tokenize_text(text)\n",
    "    build_ngram_table(tokens, n)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fff313-0f13-479b-94df-7588c19fdd3d",
   "metadata": {},
   "source": [
    "#### e) Function to digest the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ee8eaf-cf48-47a8-8d6f-ce960c181e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "digest_url <- function(url, n) {\n",
    "    res <- httr::GET(url)\n",
    "    txt <- httr::content(res, as = \"text\", encoding = \"UTF-8\")\n",
    "    digest_text(txt,n)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa4e73-ee6f-4569-9a54-9d7f7eb3f80a",
   "metadata": {},
   "source": [
    "#### f) Function that gives random start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d51f3142-2014-44b5-a186-7291893f6ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_start <- function(tbl, sep = \"\\x1f\") {\n",
    "    keys <- ls(envir = tbl, all.names=TRUE)\n",
    "    if (length(keys)==0) stop(\"No n-grams available. Digest text first.\")\n",
    "    picked <- sample(keys, 1)\n",
    "    strsplit(picked, sep, fixed=TRUE)[[1]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998fb24-f2d6-41bc-a751-1f6accd3411f",
   "metadata": {},
   "source": [
    "#### g) Function to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2925ec57-802c-4a9a-a583-a4522121e911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_next_word <- function(tbl, ngram, sep = \"\\x1f\") {\n",
    "    key <- paste(ngram, collapse = sep)\n",
    "    counts <- if(!is.null(tbl[[key]])) tbl[[key]] else integer(0)\n",
    "    if (length(counts) == 0) return(NA_character_)\n",
    "    sample(names(counts), size=1, prob=as.numeric(counts))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f4002-4932-42c4-a4af-8689293a5857",
   "metadata": {},
   "source": [
    "#### h) Function that puts everything together. Specify that if the user does not give a start word, then the random start will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cf9eb1c-dd80-4a5a-b2a2-07dcfc56e95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_ngram_generator <- function(tbl, n, sep = \"\\x1f\") {\n",
    "    force(tbl); n <- as.integer(n); force(sep)\n",
    "    function(start_words = NULL, length = 10L) {\n",
    "        if ((is.null(start_words)) || length(start_words) != n - 1L) {\n",
    "            start_words <- random_start(tbl, sep=sep)\n",
    "        }\n",
    "        word_sequence <- start_words\n",
    "        for (i in seq_len(max(0L, length - length(start_words)))) {\n",
    "            ngram <- tail(word_sequence, n - 1L)\n",
    "            next_word <- predict_next_word(tbl, ngram, sep=sep)\n",
    "            if (is.na(next_word)) break\n",
    "            word_sequence <- c(word_sequence, next_word)\n",
    "        }\n",
    "        paste(word_sequence, collapse= \" \")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b742c67-907c-4bc7-8df1-c84fa65a7554",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "#### For this question, set `seed=2025`.\n",
    "#### a) Test your model using a text file of [Grimm's Fairy Tails](https://www.gutenberg.org/cache/epub/2591/pg2591.txt)\n",
    "#### i) Using n=3, with the start word(s) \"the king\", with length=15. \n",
    "#### ii) Using n=3, with no start word, with length=15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4d65890-db0e-4301-bb03-a9f21d9814ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url <- \"https://www.gutenberg.org/cache/epub/2591/pg2591.txt\"\n",
    "tbl.a <- digest_url(url, n=3)\n",
    "gen.a <- make_ngram_generator(tbl.a, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f9ec215-5f1e-456a-ab19-19a4ecc4929c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"the king and queen also awoke and he could bear it no longer and you\"\n"
     ]
    }
   ],
   "source": [
    "# i) start words\n",
    "print(gen.a(start_words = c(\"the\", \"king\"), length=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "668e9dcb-aa1a-4cf4-9900-4321ef049c27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"that from time to hold our marriage feast look do you bring i bring nothing\"\n"
     ]
    }
   ],
   "source": [
    "# ii) no start word\n",
    "print(gen.a(length=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04b167-7f2c-4e0f-88e7-86ba5e8d74cc",
   "metadata": {},
   "source": [
    "#### b) Test your model using a text file of [Ancient Armour and Weapons in Europe](https://www.gutenberg.org/cache/epub/46342/pg46342.txt)\n",
    "#### i) Using n=3, with the start word(s) \"the king\", with length=15. \n",
    "#### ii) Using n=3, with no start word, with length=15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf7d0d2f-d779-4892-ad65-f72f51254abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url2 <- \"https://www.gutenberg.org/cache/epub/46342/pg46342.txt\"\n",
    "tbl.b <- digest_url(url2, n=3)\n",
    "gen.b <- make_ngram_generator(tbl.b, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce1e94d1-2728-427a-a962-37e729d78ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"the king of england concerning which a gang of colliers raise the hue against them\"\n"
     ]
    }
   ],
   "source": [
    "# i) start words\n",
    "print(gen.b(start_words = c(\"the\", \"king\"), length=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fa6a911-65cd-4707-92fb-38b4e3910771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"prince adds his biographer were similarly defended except that it was continued according to the\"\n"
     ]
    }
   ],
   "source": [
    "# ii) no start word\n",
    "print(gen.b(length=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb37ad-8e7c-4e62-afc0-ba46d46401fc",
   "metadata": {},
   "source": [
    "#### c) Explain in 1-2 sentences the difference in content generated from each source."
   ]
  },
  {
   "cell_type": "raw",
   "id": "79ccf911-a49f-471b-94c3-cfee35a3704d",
   "metadata": {},
   "source": [
    "The choice of words are different between the two sets of contents. The sentences generated form the fairy tails tends to use easier vocabularies whereas the armour book have more advanced vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e45972-f441-4d07-9073-fcddd6146cbd",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "#### a) What is a language learning model? \n",
    "#### b) Imagine the internet goes down and you can't run to your favorite language model for help. How do you run one locally?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0f6e547-feee-4b75-b089-16ef5ea35296",
   "metadata": {},
   "source": [
    "a) Language learning model are language models that have been trained using a lot of existing text. \n",
    "b) You can run a language learning model locally by pulling the trained model on your local machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a743b-f814-4a53-96e6-8bccb3d34ab8",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "#### Explain what the following vocab words mean in the context of typing `mkdir project` into the command line. If the term doesn't apply to this command, give the definition and/or an example.\n",
    "| Term | Meaning |  \n",
    "|------|---------|\n",
    "| **Shell** | Program that enables interaction between user and all functionalities of a system. |\n",
    "| **Terminal emulator** | The place where the shell sits. |\n",
    "| **Process** | Something that is running on the computer. |\n",
    "| **Signal** | Things that can be sent to the processes to tell them to do somthing. |\n",
    "| **Standard input** | The standard forms of input tha can be read by the process. |\n",
    "| **Standard output** | The standard forms of output tha is written by the process. |\n",
    "| **Command line argument** | Things that we pass to a process when we start it. |\n",
    "| **The environment** | All the things that a process can see. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332ff27-ca3f-4f7e-b4b9-07ead0358dd2",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "#### Consider the following command `find . -iname \"*.R\" | xargs grep read_csv`.\n",
    "#### a) What are the programs?\n",
    "#### b) Explain what this command is doing, part by part."
   ]
  },
  {
   "cell_type": "raw",
   "id": "28b635cb-8530-457f-9cae-8d1038beb72b",
   "metadata": {},
   "source": [
    "a) Find, xargs, and grep. \n",
    "b) It first search for all R files in this directory. (find . -iname \"*.R\")\n",
    "    Then, use the standard output as the standard input for next argument. (|)\n",
    "    Finally, it search for \"read_csv\" in all of the files selected. (xargs grep read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69771ac7-865e-4d82-aa25-a39e7c1ab095",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "#### Install Docker on your machine. See [here](https://github.com/rjenki/BIOS512/blob/main/lecture18/docker_install.md) for instructions. \n",
    "#### a) Show the response when you run `docker run hello-world`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "caa2cae4-e635-4e94-9578-689a8595ca04",
   "metadata": {},
   "source": [
    "Hello from Docker!\n",
    "This message shows that your installation appears to be working correctly.\n",
    "\n",
    "To generate this message, Docker took the following steps:\n",
    " 1. The Docker client contacted the Docker daemon.\n",
    " 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n",
    "    (arm64v8)\n",
    " 3. The Docker daemon created a new container from that image which runs the\n",
    "    executable that produces the output you are currently reading.\n",
    " 4. The Docker daemon streamed that output to the Docker client, which sent it\n",
    "    to your terminal.\n",
    "\n",
    "To try something more ambitious, you can run an Ubuntu container with:\n",
    " $ docker run -it ubuntu bash\n",
    "\n",
    "Share images, automate workflows, and more with a free Docker ID:\n",
    " https://hub.docker.com/\n",
    "\n",
    "For more examples and ideas, visit:\n",
    " https://docs.docker.com/get-started/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f44d4-d16c-470d-b513-ef77ca6a3ffc",
   "metadata": {},
   "source": [
    "#### b) Access Rstudio through a Docker container. Set your password and make sure your files show up on the Rstudio server. Type the command and the output you get below."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8440a075-15c9-4f98-947d-86a2b6f1f0eb",
   "metadata": {},
   "source": [
    "docker run -d -p 8787:8787 -v ~/Projects/:/home/rstudio/projects -e PASSWORD=\"*****\" amoselb/rstudio-m1\n",
    "\n",
    "output:\n",
    "Unable to find image 'amoselb/rstudio-m1:latest' locally\n",
    "latest: Pulling from amoselb/rstudio-m1\n",
    "bab46ae8f092: Pull complete \n",
    "c2b8ed7b26a4: Pull complete \n",
    "bcef7822d644: Pull complete \n",
    "273ca194f359: Pull complete \n",
    "64e3fba06635: Pull complete \n",
    "0ef85aa9332d: Pull complete \n",
    "3dbb433f4513: Pull complete \n",
    "f0406931adcd: Pull complete \n",
    "b91aa4d3ce56: Pull complete \n",
    "923cc17ae928: Pull complete \n",
    "13f84581a6a4: Pull complete \n",
    "aa71cdda3271: Pull complete \n",
    "a325a8d3699b: Pull complete \n",
    "7f1bbdf10974: Pull complete \n",
    "768f74323ec4: Pull complete \n",
    "b0508151705b: Pull complete \n",
    "ca55caf48640: Pull complete \n",
    "758243992752: Pull complete \n",
    "c3042a0d94b8: Pull complete \n",
    "d13bb7eab3f7: Pull complete \n",
    "Digest: sha256:59a8f2e44b024165c0619bbd8039952d6f2f5246ee5b12eb1e0acc4ce8bdb670\n",
    "Status: Downloaded newer image for amoselb/rstudio-m1:latest\n",
    "f731f10d9d7ee5116da118200ce3c2a93b108c498f53fdbcbc9032424ac3e18c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba3a64-bdce-449c-9a85-79ffe86575bb",
   "metadata": {},
   "source": [
    "#### c) How do you log in to the RStudio server?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197fa57-5ad3-473c-be31-11f5e7fd3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Open browser and go to http://localhost:8787.\n",
    "Then, it ask for username and password.\n",
    "Use rstudio as username and the set password from last command. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
